{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Walkthrough\n",
    "\n",
    "The goal of this walkthrough is to demonstrate how to train a simple yet powerful machine learning model that is widely used in industry for a variety of applications - **Logistics Regression**. The model trained in the walkthrough has a potential for scaling to massive data sets by harnessing  the computational resources you may have (one or more CPU cores, one or more GPUs, a cluster of CPUs or a cluster of GPUs), transparently via the CNTK library.\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "**Logistic Regression** is a fundamental machine learning technique that uses a linear weighted combination of features and generates probability-based predictions of different classes.  \n",
    "\n",
    "There are two basic forms of LR: **Binary LR** (with a single output that can predict two classes) and **multinomial LR** (with multiple outputs, each of which is used to predict a single class). In this walkthrough we are going to focus on **BLR**.  \n",
    "\n",
    "![LR-forms](http://www.cntk.ai/jup/logistic_neuron.jpg)\n",
    "\n",
    "In **Binary Logistic Regression**, the input features are each scaled by an associated weight and summed together.  The sum is passed through a squashing (aka activation) function and generates an output in [0,1].  This output value (which can be thought of as a probability) is then compared with a threshold (such as 0.5) to produce a binary label (0 or 1).  This technique supports only classification problems with two output classes, hence the name binary LR.  In the binary LR example shown above, the `sigmoid` function is used as the squashing function.\n",
    "\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}} \\text{,  where  } z = \\sum_{i=1}^n w_i \\times x_i + b = \\textbf{w}^T \\cdot \\textbf{x} + b $$\n",
    "\n",
    "\n",
    "\n",
    "For **Logistics Regression** model training means estimating the model's parameters - **w** and **b** - so that the probabilities generated by the model are as close as possible to the observed labels. \n",
    "\n",
    "This is achieved by minimizing the *cost* or *loss* function, which \"measures\" the difference between predictions generated by the learnt model vs. the training data set.\n",
    "\n",
    "`Cross-entropy` is a popular function to measure the loss. It is defined as:\n",
    "\n",
    "$$ H(w) = -\\frac{1}{N} \\sum_{j=1}^N (y^{'}_j\\log(y_j) + (1 - y^{'}_j)\\log(1 - y_j))  $$  \n",
    "\n",
    "where $y$ is our predicted probability from `sigmoid` function and $y'$ represents the ground-truth label. \n",
    "\n",
    "The trainer strives to reduce the `loss` function by different optimization approaches, Stochastic Gradient Descent (`sgd`) being one of the most popular one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer would calculate the `loss` or error between the predicted label against the corresponding ground-truth label and using `gradient-decent` generate a new set model parameters in a single iteration. \n",
    "\n",
    "The aforementioned model parameter update using a single observation at a time is attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations.\n",
    "An intermediate ground is to load a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "In the extreme case we can use all the observations in the training dataset.\n",
    "\n",
    "With minibatches we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "One of the key parameter for optimization is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scenario overview\n",
    "\n",
    "A cancer hospital has provided data and wants us to determine if a patient has  malignant cancer vs. a benign growth. This is known as a classification problem. To help classify each patient, we are given their age and the size of the tumor. Intuitively, one can imagine that younger patients and/or patient with small tumor size are less likely to have malignant cancer. The data set simulates this application where the each observation is a patient represented as a dot (in the plot below) where red color indicates malignant and blue indicates benign disease. Note: This is a toy example for learning, in real life there are large number of features from different tests/examination sources and doctors'  experience that play into the diagnosis/treatment decision for a patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Select the right target device \n",
    "# C.device.try_set_default_device(cntk.device.cpu())\n",
    "# C.device.try_set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "We are using a simulated data set with two features and a binary label (benign:0 or malignant:1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('../../Data/cancer.csv', delimiter=',', skiprows=1, dtype=np.float32)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "colors = ['r' if l == 1. else 'b' for l in dataset[:,[2]]]\n",
    "plt.scatter(dataset[:,[0]], dataset[:,[1]], c=colors)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Tumor size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation features and labels\n",
    "- Scale features to [0,1] range\n",
    "- Reorganize into NumPy contiguous arrays to minimize costly conversions between NumPy and CTNK internal formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    training = np.copy(dataset)\n",
    "    np.random.shuffle(training)\n",
    "    training[:,[0,1]] /= np.max(training[:,[0,1]])\n",
    "    index = int(len(training) * .7) \n",
    "    \n",
    "    return np.ascontiguousarray(training[0:index, [0,1]]), \\\n",
    "           np.ascontiguousarray(training[0:index, [2]]), \\\n",
    "           np.ascontiguousarray(training[index+1:len(training), [0,1]]), \\\n",
    "           np.ascontiguousarray(training[index+1:len(training), [2]])\n",
    "\n",
    "training_features, training_labels, validation_features, validation_labels = preprocess_dataset(dataset)\n",
    "\n",
    "print(training_features[0:5])\n",
    "print(\"\")\n",
    "print(training_labels[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "_Note that we have focused on making the code simple and straigthforward. It is not optimized for ultimate performance and may be numericaly unstable. In other labs we will follow best practices and guidelines for developing high performance and stable models._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a computational network\n",
    "\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\text{,  where  } z = \\sum_{i=1}^n w_i \\times x_i + b = \\textbf{w}^T \\cdot \\textbf{x} + b $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1 \n",
    "\n",
    "X = C.input(shape=(input_dim))\n",
    "w = C.parameter(shape=(input_dim, output_dim))\n",
    "b = C.parameter(shape=(output_dim))\n",
    "y = C.sigmoid(X @ w + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a loss function\n",
    "\n",
    "\n",
    "$$ H(w) = -\\frac{1}{N} \\sum_{j=1}^N (y^{'}_j\\log(y_j) + (1 - y^{'}_j)\\log(1 - y_j))  $$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ = C.input((output_dim))\n",
    "loss = -C.reduce_sum(y_ * C.log(y) + (1 - y_) * C.log(1 - y), axis=C.Axis.all_axes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the SGD trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n",
    "progress_printer = ProgressPrinter(20)\n",
    "learning_rate = 0.02\n",
    "lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.sample)\n",
    "learner = C.sgd(y.parameters, lr_schedule)\n",
    "trainer = C.Trainer(y, (loss), [learner], [progress_printer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_sweeps = 500 \n",
    "for i in range(0, num_of_sweeps):\n",
    "    trainer.train_minibatch({X : training_features, y_ : training_labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evalulation\n",
    "\n",
    "### Plot the learned decision boundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = y.parameters[0].value\n",
    "bias = y.parameters[1].value\n",
    "colors = ['r' if l == 1. else 'b' for l in training_labels]\n",
    "plt.scatter(training_features[:,[0]], training_features[:,[1]], c=colors)\n",
    "plt.plot([0, -bias[0]/weights[1][0]],\n",
    "         [-bias[0]/weights[0][0], 0], c = 'g', lw =2)\n",
    "plt.axis([0.2, 1.1, 0.2, 0.9])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Tumor size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the model on the validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = y.eval({X: validation_features})\n",
    "print(result[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted_labels = np.round(result)\n",
    "print(confusion_matrix(validation_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
